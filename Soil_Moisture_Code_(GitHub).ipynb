{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/itsaliaze/SMAP-Downscaling.git"
      ],
      "metadata": {
        "id": "CMz3Rh7lVfYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Libraries"
      ],
      "metadata": {
        "id": "ms_A5zaWXVet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilkOq77lGvE1"
      },
      "outputs": [],
      "source": [
        "# Installing necessary libraries for geospatial data handling\n",
        "!pip install geopandas geojson fiona georasters rioxarray xarray raster2xyz rasterio kerastuner\n",
        "\n",
        "# Importing required libraries\n",
        "import fiona, geojson, folium, ee, numpy as np\n",
        "import geopandas as gpd, georasters as gr, matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "import geemap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing GEE"
      ],
      "metadata": {
        "id": "Tenbf60MXpzw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gwH8g1z7IS08",
        "outputId": "e63a3f8e-bfbf-485c-cdf7-c56c1ba8a903"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize Google Earth Engine (GEE)\n",
        "ee.Authenticate()  # Authenticate GEE access\n",
        "ee.Initialize(project='august-edge-339813')  # Initialize GEE project\n",
        "geemap.ee_initialize(project='august-edge-339813')  # Initialize geemap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Period of Interest"
      ],
      "metadata": {
        "id": "ZeglXcSgZGzv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u6sQ70AIsVT"
      },
      "outputs": [],
      "source": [
        "# Period of interest\n",
        "start_date = '2023-11-03'\n",
        "end_date = '2023-12-05'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading and Plotting the Study Area Shapefile"
      ],
      "metadata": {
        "id": "TvEnjtkbZTk8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe6-FdhkI396"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the shapefile\n",
        "CV = gpd.read_file(\"/content/Alluvial_Bnd.shp\")\n",
        "\n",
        "# Plot the shapefile without the surrounding box\n",
        "fig, ax = plt.subplots()\n",
        "CV.plot(ax=ax)\n",
        "\n",
        "# Remove the surrounding box\n",
        "ax.set_frame_on(False)\n",
        "\n",
        "# Remove x-axis and y-axis tick marks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Shapefile to FeatureCollection and Extracting Geometry"
      ],
      "metadata": {
        "id": "oPysinoVZ-rr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wrojlQLJGyg"
      },
      "outputs": [],
      "source": [
        "# Convert the GeoDataFrame to an ee.FeatureCollection\n",
        "fc = geemap.geopandas_to_ee(CV)\n",
        "# Get the first feature from the FeatureCollection\n",
        "feature_ = ee.Feature(fc.first())\n",
        "# Extract the geometry from the feature\n",
        "CV = feature_.geometry()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Predictor Variables"
      ],
      "metadata": {
        "id": "dPG70kjiaLmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2Qg2z9yJkZa"
      },
      "outputs": [],
      "source": [
        "# SMAP Surface Soil Moisture\n",
        "SMAP = ee.ImageCollection('NASA/SMAP/SPL4SMGP/007') \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select('sm_surface').mean().clip(CV)\n",
        "\n",
        "# SAR Imagery (VH & VV Polarization)\n",
        "VH = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "    .filterDate(start_date, end_date).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')).select('VH') \\\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW')).mean().clip(CV)\n",
        "VV = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "    .filterDate(start_date, end_date).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV') \\\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW')).mean().clip(CV)\n",
        "\n",
        "# Precipitation (Total Precipitation during the period)\n",
        "CHIRPS = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY') \\\n",
        "    .filterDate(start_date, end_date).sum().select('precipitation').clip(CV)\n",
        "\n",
        "# Land Surface Temperature (Landsat)\n",
        "LST = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
        "    .filterDate(start_date, end_date).mean().select('ST_B10').clip(CV)\n",
        "\n",
        "# Brightness Temperature\n",
        "BT = ee.ImageCollection('NOAA/CDR/PATMOSX/V53') \\\n",
        "    .filterDate(start_date, end_date).mean().select('temp_12_0um').clip(CV)\n",
        "\n",
        "# MODIS Daytime Temperature\n",
        "Temp = ee.ImageCollection('MODIS/061/MOD11A2') \\\n",
        "    .filterDate(start_date, end_date).mean().select('LST_Day_1km').clip(CV)\n",
        "\n",
        "# Evapotranspiration (MODIS)\n",
        "ET = ee.ImageCollection('MODIS/061/MOD16A2') \\\n",
        "    .filterDate(start_date, end_date).mean().select('ET').clip(CV)\n",
        "\n",
        "# Elevation (DEM)\n",
        "DEM = ee.Image('USGS/3DEP/10m').select('elevation').clip(CV)\n",
        "\n",
        "# Sentinel-2\n",
        "# Define a function to mask clouds using the Sentinel-2 QA60 band\n",
        "def mask_s2_clouds(image):\n",
        "    # Use QA60 band to mask out clouds (1 indicates cloud, 0 indicates no cloud)\n",
        "    cloud_mask = image.select('QA60').eq(0)\n",
        "    return image.updateMask(cloud_mask)\n",
        "# Sentinel-2 with cloud filtering based on CLOUDY_PIXEL_PERCENTAGE\n",
        "Sentinel2 = (\n",
        "    ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
        "    .filterDate(start_date, end_date)  # Define the date range\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))  # Keep images with less than 20% cloud cover\n",
        "    .select('B2', 'B3', 'B4', 'B5', 'B6', 'B8', 'B11', 'B12')  # Select relevant bands\n",
        "    .map(mask_s2_clouds)  # Apply cloud mask\n",
        "    .mean()  # Compute the mean of the images\n",
        "    .clip(CV)  # Clip to the study area\n",
        ")\n",
        "\n",
        "# Cropland Data Layer (USDA)\n",
        "Crop = ee.ImageCollection('USDA/NASS/CDL').filterDate(start_date, end_date).select('cropland').first().clip(CV)\n",
        "\n",
        "# Land Cover (Dynamic World)\n",
        "LC = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1').filterDate(start_date, end_date).select('label').mean().clip(CV)\n",
        "\n",
        "# Slope (from DEM)\n",
        "SLOPE = ee.Terrain.slope(DEM).clip(CV)\n",
        "\n",
        "# Topographic Roughness (Standard deviation of DEM)\n",
        "ROUGHNESS = DEM.reduceNeighborhood(ee.Reducer.stdDev(), ee.Kernel.square(5)).clip(CV)\n",
        "\n",
        "#Soil Properties\n",
        "silt = ee.ImageCollection(\"projects/sat-io/open-datasets/polaris/silt_mean\").first().clip(CV)\n",
        "clay = ee.ImageCollection(\"projects/sat-io/open-datasets/polaris/clay_mean\").first().clip(CV)\n",
        "sand = ee.ImageCollection(\"projects/sat-io/open-datasets/polaris/sand_mean\").first().clip(CV)\n",
        "om = ee.ImageCollection('projects/sat-io/open-datasets/polaris/om_mean').first().clip(CV)\n",
        "bd = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').first().clip(CV)\n",
        "\n",
        "# Topographic Wetness Index (TWI)\n",
        "cell_size = 10.2 # Define cell size (resolution) in meters\n",
        "weights = ee.List.repeat(ee.List.repeat(1, 3), 3)  # Define a 3x3 kernel\n",
        "kernel = ee.Kernel.fixed(3, 3, weights)  # Create fixed kernel\n",
        "flow_accumulation = SLOPE.gt(0).reduceNeighborhood(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    kernel=kernel\n",
        ")\n",
        "TWI = flow_accumulation.log().divide(SLOPE.tan())\n",
        "\n",
        "# Function to calculate vegetation indices for each image\n",
        "def calculate_indices(image):\n",
        "    # NDVI, EVI, SAVI, NDWI, LAI calculation\n",
        "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    evi = image.expression('2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))', {\n",
        "        'NIR': image.select('B8'),\n",
        "        'Red': image.select('B4'),\n",
        "        'Blue': image.select('B2')\n",
        "    }).rename('EVI')\n",
        "    savi = image.expression('(1 + L) * ((NIR - Red) / (NIR + Red + L))', {\n",
        "        'NIR': image.select('B8'),\n",
        "        'Red': image.select('B4'),\n",
        "        'L': 0.5\n",
        "    }).rename('SAVI')\n",
        "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
        "    lai = image.expression('3.618 * ((NIR - Red) / (0.5 + NIR + Red))', {\n",
        "        'NIR': image.select('B8'),\n",
        "        'Red': image.select('B4')\n",
        "    }).rename('LAI')\n",
        "    return image.addBands([ndvi, evi, savi, ndwi, lai])\n",
        "\n",
        "# Apply the vegetation indices calculation to Sentinel-2 imagery\n",
        "S2 = calculate_indices(Sentinel2)\n",
        "\n",
        "# Extract individual indices\n",
        "NDVI = S2.select('NDVI')\n",
        "EVI = S2.select('EVI')\n",
        "SAVI = S2.select('SAVI')\n",
        "NDWI = S2.select('NDWI')\n",
        "LAI = S2.select('LAI')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display All Variables in GEEMap"
      ],
      "metadata": {
        "id": "IQvBoC-ZevsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geemap\n",
        "\n",
        "# Create a geemap instance\n",
        "Map = geemap.Map()\n",
        "\n",
        "# Add the Central Valley Geometry object to the map\n",
        "Map.addLayer(CV, {}, 'ROI', True)\n",
        "\n",
        "# Add the SMAP layer\n",
        "Map.addLayer(SMAP, {'min': 0, 'max': 1}, 'SMAP', True)\n",
        "\n",
        "# Add the SAR imagery (VH and VV)\n",
        "Map.addLayer(VH, {'min': -50, 'max': 0}, 'VH', True)\n",
        "Map.addLayer(VV, {'min': -50, 'max': 0}, 'VV', True)\n",
        "\n",
        "# Add the precipitation imagery (CHIRPS)\n",
        "Map.addLayer(CHIRPS, {'min': 0, 'max': 40}, 'Precipitation', True)\n",
        "\n",
        "# Add the Land Surface Temperature (LST)\n",
        "Map.addLayer(LST, {'min': 0, 'max': 83323}, 'Land Surface Temperature', True)\n",
        "\n",
        "# Add the Evapotranspiration (ET)\n",
        "Map.addLayer(ET, {'min': 20, 'max': 150}, 'Evapotranspiration', True)\n",
        "\n",
        "# Add the Sentinel-2 imagery with bands and indices\n",
        "Map.addLayer(S2, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3500}, 'Sentinel-2', True)\n",
        "\n",
        "# Add the Digital Elevation Model (DEM)\n",
        "Map.addLayer(DEM, {'min': 0, 'max': 4500}, 'DEM', True)\n",
        "\n",
        "# Add the Slope image\n",
        "Map.addLayer(SLOPE, {'min': 0, 'max': 40}, 'SLOPE', True)\n",
        "\n",
        "# Add the TWI (Topographic Wetness Index)\n",
        "Map.addLayer(TWI, {'min': -3, 'max': 30}, 'TWI', True)\n",
        "\n",
        "# Add the Roughness layer\n",
        "Map.addLayer(ROUGHNESS, {'min': 0, 'max': 800}, 'ROUGHNESS', True)\n",
        "\n",
        "# Set visualization parameters for the vegetation indices\n",
        "vis_params_indexes = {'min': -1, 'max': 1, 'palette': ['blue', 'white', 'green', 'yellow', 'blue']}\n",
        "\n",
        "# Add the vegetation indices to the map\n",
        "Map.addLayer(S2.select('NDVI'), vis_params_indexes, 'NDVI', True)\n",
        "Map.addLayer(S2.select('EVI'), vis_params_indexes, 'EVI', True)\n",
        "Map.addLayer(S2.select('SAVI'), vis_params_indexes, 'SAVI', True)\n",
        "Map.addLayer(S2.select('NDWI'), vis_params_indexes, 'NDWI', True)\n",
        "Map.addLayer(S2.select('LAI'), vis_params_indexes, 'LAI', True)\n",
        "\n",
        "# Add soil properties (e.g., sand, silt, clay, organic matter, and bulk density)\n",
        "Map.addLayer(sand, {'min': 5, 'max': 90}, 'Sand', True)\n",
        "Map.addLayer(silt, {'min': 2, 'max': 80}, 'Silt', True)\n",
        "Map.addLayer(clay, {'min': 3, 'max': 55}, 'Clay', True)\n",
        "Map.addLayer(om, {'min': -0.8, 'max': 1.8}, 'Organic Matter', True)\n",
        "Map.addLayer(bd, {'min': 0.6, 'max': 1.6}, 'Bulk Density', True)\n",
        "\n",
        "# Add the crop layer\n",
        "Map.addLayer(Crop, {'min': 0, 'max': 254}, 'Crop', True)\n",
        "\n",
        "# Center the map around California's Central Valley\n",
        "Map.centerObject(CV, zoom=10)\n",
        "\n",
        "# Display the map\n",
        "Map"
      ],
      "metadata": {
        "id": "JXCNIbQ5DOQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNPHFuNMUsou"
      },
      "source": [
        "DATA PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifb_QY40KF1k"
      },
      "source": [
        "Resampling data to different resolutions (1000 × 1000 m, 100 × 100 m, 50 × 50 m) and the target resolution (30 × 30 m) to prepare the training and contextual data for the downscaling model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bBu9v1z4Yyuj",
        "outputId": "f94ff083-4b3a-4a9d-ea73-468b64a2d580"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <style>\n",
              "                .geemap-dark {\n",
              "                    --jp-widgets-color: white;\n",
              "                    --jp-widgets-label-color: white;\n",
              "                    --jp-ui-font-color1: white;\n",
              "                    --jp-layout-color2: #454545;\n",
              "                    background-color: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-dark .jupyter-button {\n",
              "                    --jp-layout-color3: #383838;\n",
              "                }\n",
              "\n",
              "                .geemap-colab {\n",
              "                    background-color: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "\n",
              "                .geemap-colab .jupyter-button {\n",
              "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
              "                }\n",
              "            </style>\n",
              "            "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Function to reproject and resample the raster layers\n",
        "def resample_to_resolution(image, resolution, projection='EPSG:4326'):\n",
        "    \"\"\"\n",
        "    This function reprojects and resamples a given image to the desired resolution\n",
        "    using bicubic resampling.\n",
        "\n",
        "    Parameters:\n",
        "    - image: The image to be resampled.\n",
        "    - resolution: The target resolution (in meters).\n",
        "    - projection: The projection system to use (default is 'EPSG:4326').\n",
        "\n",
        "    Returns:\n",
        "    - Resampled image.\n",
        "    \"\"\"\n",
        "    return image.reproject(crs=projection, scale=resolution).resample('bicubic').reduceResolution(reducer=ee.Reducer.mean(), maxPixels=10240)\n",
        "\n",
        "# Selecting the common spatial resolution for all the variables\n",
        "resolution = 1000\n",
        "# Selecting the common projection system (EPSG:4326)\n",
        "projection = 'EPSG:4326'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproject and resample the variable rasters to the desired resolution\n",
        "SMAP_1000m = resample_to_resolution(SMAP, resolution)\n",
        "VH_1000m = resample_to_resolution(VH, resolution)\n",
        "VV_1000m = resample_to_resolution(VV, resolution)\n",
        "CHIRPS_1000m = resample_to_resolution(CHIRPS, resolution)\n",
        "ET_1000m = resample_to_resolution(ET, resolution)\n",
        "DEM_1000m = resample_to_resolution(DEM, resolution)\n",
        "SLOPE_1000m = resample_to_resolution(SLOPE, resolution)\n",
        "TWI_1000m = resample_to_resolution(TWI, resolution)\n",
        "LST_1000m = resample_to_resolution(LST, resolution)\n",
        "NDVI_1000m = resample_to_resolution(S2.select('NDVI'), resolution)\n",
        "EVI_1000m = resample_to_resolution(S2.select('EVI'), resolution)\n",
        "SAVI_1000m = resample_to_resolution(S2.select('SAVI'), resolution)\n",
        "NDWI_1000m = resample_to_resolution(S2.select('NDWI'), resolution)\n",
        "LAI_1000m = resample_to_resolution(S2.select('LAI'), resolution)\n",
        "Crop_1000m = resample_to_resolution(Crop, resolution)\n",
        "LC_1000m = resample_to_resolution(LC, resolution)\n",
        "SAND_1000m = resample_to_resolution(sand, resolution)\n",
        "SILT_1000m = resample_to_resolution(silt, resolution)\n",
        "CLAY_1000m = resample_to_resolution(clay, resolution)\n",
        "BD_1000m = resample_to_resolution(bd, resolution)\n",
        "OM_1000m = resample_to_resolution(om, resolution)"
      ],
      "metadata": {
        "id": "nMzGrAYJZAc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the data"
      ],
      "metadata": {
        "id": "s2aQpltbaody"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYPWLGx7KH0N"
      },
      "outputs": [],
      "source": [
        "# List of all predictor variables\n",
        "predictors_1000m = [\n",
        "    SMAP_1000m, VH_1000m, VV_1000m, CHIRPS_1000m, ET_1000m, DEM_1000m,\n",
        "    SLOPE_1000m, TWI_1000m, LST_1000m, NDVI_1000m, EVI_1000m, SAVI_1000m,\n",
        "    NDWI_1000m, LAI_1000m, Crop_1000m, LC_1000m, SAND_1000m, SILT_1000m,\n",
        "    CLAY_1000m, BD_1000m, OM_1000m\n",
        "]\n",
        "\n",
        "# Export each variable to Google Drive\n",
        "for idx, variable in enumerate(predictors_100m):\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=variable,\n",
        "        description=f'export_{variable.getInfo()[\"id\"]}',  # Unique description for each variable\n",
        "        folder='SMAP_DOWNSCALING',\n",
        "        region=CV,\n",
        "        fileFormat='GeoTIFF',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "\n",
        "    task.start()  # Start the export task\n",
        "    print(f\"Exporting {variable.getInfo()['id']}...\")  # Inform the user which variable is being exported\n",
        "\n",
        "# Monitor the export status\n",
        "for task in ee.batch.Task.list():\n",
        "    print(task.status())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google Drive"
      ],
      "metadata": {
        "id": "BfZcqDeTbaYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPgYysgKN6GQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzG7YErYVITF"
      },
      "source": [
        "Preparing the df for a specific resolution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from raster2xyz.raster2xyz import Raster2xyz\n",
        "\n",
        "# Initialize Raster2xyz object\n",
        "rtxyz = Raster2xyz()\n",
        "\n",
        "# Convert each raster to CSV and load as DataFrame\n",
        "\n",
        "# SMAP 1000m\n",
        "input_raster_smap_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SMAP_D_1000m.tif'\n",
        "output_csv_smap_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SMAP_D_1000m.csv'\n",
        "rtxyz.translate(input_raster_smap_1000m, output_csv_smap_1000m)\n",
        "smap_raster_df_1000m = pd.read_csv(output_csv_smap_1000m, skiprows=1, names=['x', 'y', 'SMAP']).dropna()\n",
        "\n",
        "# VH 1000m\n",
        "input_raster_vh_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/VH_1000m.tif'\n",
        "output_csv_vh_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/VH_1000m.csv'\n",
        "rtxyz.translate(input_raster_vh_1000m, output_csv_vh_1000m)\n",
        "vh_raster_df_1000m = pd.read_csv(output_csv_vh_1000m, skiprows=1, names=['x', 'y', 'VH']).dropna()\n",
        "\n",
        "# VV 1000m\n",
        "input_raster_vv_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/VV_1000m.tif'\n",
        "output_csv_vv_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/VV_1000m.csv'\n",
        "rtxyz.translate(input_raster_vv_1000m, output_csv_vv_1000m)\n",
        "vv_raster_df_1000m = pd.read_csv(output_csv_vv_1000m, skiprows=1, names=['x', 'y', 'VV']).dropna()\n",
        "\n",
        "# CHIRPS 1000m\n",
        "input_raster_chirps_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/CHIRPS_1000m.tif'\n",
        "output_csv_chirps_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/CHIRPS_1000m.csv'\n",
        "rtxyz.translate(input_raster_chirps_1000m, output_csv_chirps_1000m)\n",
        "chirps_raster_df_1000m = pd.read_csv(output_csv_chirps_1000m, skiprows=1, names=['x', 'y', 'CHIRPS']).dropna()\n",
        "\n",
        "# LST 1000m\n",
        "input_raster_lst_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/LST_1000m.tif'\n",
        "output_csv_lst_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/LST_1000m.csv'\n",
        "rtxyz.translate(input_raster_lst_1000m, output_csv_lst_1000m)\n",
        "lst_raster_df_1000m = pd.read_csv(output_csv_lst_1000m, skiprows=1, names=['x', 'y', 'LST']).dropna()\n",
        "\n",
        "# BT 1000m\n",
        "input_raster_bt_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/BT_1000m.tif'\n",
        "output_csv_bt_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/BT_1000m.csv'\n",
        "rtxyz.translate(input_raster_bt_1000m, output_csv_bt_1000m)\n",
        "bt_raster_df_1000m = pd.read_csv(output_csv_bt_1000m, skiprows=1, names=['x', 'y', 'BT']).dropna()\n",
        "\n",
        "# ET 1000m\n",
        "input_raster_et_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/ET_1000m.tif'\n",
        "output_csv_et_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/ET_1000m.csv'\n",
        "rtxyz.translate(input_raster_et_1000m, output_csv_et_1000m)\n",
        "et_raster_df_1000m = pd.read_csv(output_csv_et_1000m, skiprows=1, names=['x', 'y', 'ET']).dropna()\n",
        "\n",
        "# DEM 1000m\n",
        "input_raster_dem_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/DEM_1000m.tif'\n",
        "output_csv_dem_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/DEM_1000m.csv'\n",
        "rtxyz.translate(input_raster_dem_1000m, output_csv_dem_1000m)\n",
        "dem_raster_df_1000m = pd.read_csv(output_csv_dem_1000m, skiprows=1, names=['x', 'y', 'DEM']).dropna()\n",
        "\n",
        "# SLOPE 1000m\n",
        "input_raster_slope_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SLOPE_1000m.tif'\n",
        "output_csv_slope_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SLOPE_1000m.csv'\n",
        "rtxyz.translate(input_raster_slope_1000m, output_csv_slope_1000m)\n",
        "slope_raster_df_1000m = pd.read_csv(output_csv_slope_1000m, skiprows=1, names=['x', 'y', 'SLOPE']).dropna()\n",
        "\n",
        "# TWI 1000m\n",
        "input_raster_twi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/TWI_1000m.tif'\n",
        "output_csv_twi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/TWI_1000m.csv'\n",
        "rtxyz.translate(input_raster_twi_1000m, output_csv_twi_1000m)\n",
        "twi_raster_df_1000m = pd.read_csv(output_csv_twi_1000m, skiprows=1, names=['x', 'y', 'TWI']).dropna()\n",
        "\n",
        "# NDVI 1000m\n",
        "input_raster_ndvi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/NDVI_1000m.tif'\n",
        "output_csv_ndvi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/NDVI_1000m.csv'\n",
        "rtxyz.translate(input_raster_ndvi_1000m, output_csv_ndvi_1000m)\n",
        "ndvi_raster_df_1000m = pd.read_csv(output_csv_ndvi_1000m, skiprows=1, names=['x', 'y', 'NDVI']).dropna()\n",
        "\n",
        "# EVI 1000m\n",
        "input_raster_evi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/EVI_1000m.tif'\n",
        "output_csv_evi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/EVI_1000m.csv'\n",
        "rtxyz.translate(input_raster_evi_1000m, output_csv_evi_1000m)\n",
        "evi_raster_df_1000m = pd.read_csv(output_csv_evi_1000m, skiprows=1, names=['x', 'y', 'EVI']).dropna()\n",
        "\n",
        "# SAVI 1000m\n",
        "input_raster_savi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SAVI_1000m.tif'\n",
        "output_csv_savi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SAVI_1000m.csv'\n",
        "rtxyz.translate(input_raster_savi_1000m, output_csv_savi_1000m)\n",
        "savi_raster_df_1000m = pd.read_csv(output_csv_savi_1000m, skiprows=1, names=['x', 'y', 'SAVI']).dropna()\n",
        "\n",
        "# NDWI 1000m\n",
        "input_raster_ndwi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/NDWI_1000m.tif'\n",
        "output_csv_ndwi_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/NDWI_1000m.csv'\n",
        "rtxyz.translate(input_raster_ndwi_1000m, output_csv_ndwi_1000m)\n",
        "ndwi_raster_df_1000m = pd.read_csv(output_csv_ndwi_1000m, skiprows=1, names=['x', 'y', 'NDWI']).dropna()\n",
        "\n",
        "# LAI 1000m\n",
        "input_raster_lai_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/LAI_1000m.tif'\n",
        "output_csv_lai_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/LAI_1000m.csv'\n",
        "rtxyz.translate(input_raster_lai_1000m, output_csv_lai_1000m)\n",
        "lai_raster_df_1000m = pd.read_csv(output_csv_lai_1000m, skiprows=1, names=['x', 'y', 'LAI']).dropna()\n",
        "\n",
        "# Crop 1000m\n",
        "input_raster_crop_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/Crop_1000m.tif'\n",
        "output_csv_crop_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/Crop_1000m.csv'\n",
        "rtxyz.translate(input_raster_crop_1000m, output_csv_crop_1000m)\n",
        "crop_raster_df_1000m = pd.read_csv(output_csv_crop_1000m, skiprows=1, names=['x', 'y', 'Crop']).dropna()\n",
        "\n",
        "# LC 1000m\n",
        "input_raster_lc_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/LC_1000m.tif'\n",
        "output_csv_lc_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/LC_1000m.csv'\n",
        "rtxyz.translate(input_raster_lc_1000m, output_csv_lc_1000m)\n",
        "lc_raster_df_1000m = pd.read_csv(output_csv_lc_1000m, skiprows=1, names=['x', 'y', 'LC']).dropna()\n",
        "\n",
        "# SAND 1000m\n",
        "input_raster_sand_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SAND_1000m.tif'\n",
        "output_csv_sand_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SAND_1000m.csv'\n",
        "rtxyz.translate(input_raster_sand_1000m, output_csv_sand_1000m)\n",
        "sand_raster_df_1000m = pd.read_csv(output_csv_sand_1000m, skiprows=1, names=['x', 'y', 'SAND']).dropna()\n",
        "\n",
        "# SILT 1000m\n",
        "input_raster_silt_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SILT_1000m.tif'\n",
        "output_csv_silt_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/SILT_1000m.csv'\n",
        "rtxyz.translate(input_raster_silt_1000m, output_csv_silt_1000m)\n",
        "silt_raster_df_1000m = pd.read_csv(output_csv_silt_1000m, skiprows=1, names=['x', 'y', 'SILT']).dropna()\n",
        "\n",
        "# CLAY 1000m\n",
        "input_raster_clay_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/CLAY_1000m.tif'\n",
        "output_csv_clay_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/CLAY_1000m.csv'\n",
        "rtxyz.translate(input_raster_clay_1000m, output_csv_clay_1000m)\n",
        "clay_raster_df_1000m = pd.read_csv(output_csv_clay_1000m, skiprows=1, names=['x', 'y', 'CLAY']).dropna()\n",
        "\n",
        "# BD 1000m\n",
        "input_raster_bd_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/BD_1000m.tif'\n",
        "output_csv_bd_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/BD_1000m.csv'\n",
        "rtxyz.translate(input_raster_bd_1000m, output_csv_bd_1000m)\n",
        "bd_raster_df_1000m = pd.read_csv(output_csv_bd_1000m, skiprows=1, names=['x', 'y', 'BD']).dropna()\n",
        "\n",
        "# OM 1000m\n",
        "input_raster_om_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/OM_1000m.tif'\n",
        "output_csv_om_1000m = '/content/drive/MyDrive/SMAP_DOWNSCALING_CV/OM_1000m.csv'\n",
        "rtxyz.translate(input_raster_om_1000m, output_csv_om_1000m)\n",
        "om_raster_df_1000m = pd.read_csv(output_csv_om_1000m, skiprows=1, names=['x', 'y', 'OM']).dropna()"
      ],
      "metadata": {
        "id": "onCjLncMfp7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging Dfs"
      ],
      "metadata": {
        "id": "anClR4z2urel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL46rA7Vfmn-"
      },
      "outputs": [],
      "source": [
        "final_df_1000m = (\n",
        "    smap_raster_df_1000m\n",
        "    .merge(vh_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(vv_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(chirps_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(et_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(dem_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(slope_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(twi_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(lst_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(ndvi_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(evi_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(savi_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(ndwi_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(lai_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(crop_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(lc_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(sand_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(silt_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(clay_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(bd_raster_df_1000m, on=['x', 'y'])\n",
        "    .merge(om_raster_df_1000m, on=['x', 'y'])\n",
        ")\n",
        "\n",
        "# Output the shape of the final DataFrame and ensure numeric conversion\n",
        "print(final_df_1000m.shape)\n",
        "\n",
        "# Ensure the 'SMAP' column is numeric, coercing any non-numeric values to NaN\n",
        "final_df_1000m['SMAP'] = pd.to_numeric(final_df_1000m['SMAP'], errors='coerce')\n",
        "\n",
        "# Display the first few rows of the final DataFrame\n",
        "final_df_1000m.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenation of all the resolutions"
      ],
      "metadata": {
        "id": "-dIL0xo114EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all dataframes by adding new rows\n",
        "final_combined_df = pd.concat([final_df_1000m, final_df_100m, final_df_50m], ignore_index=True)\n",
        "\n",
        "# Output the shape of the final combined DataFrame\n",
        "print(final_combined_df.shape)\n",
        "\n",
        "# Display the first few rows of the combined DataFrame\n",
        "final_combined_df.head()"
      ],
      "metadata": {
        "id": "M01v9vI710-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize the data"
      ],
      "metadata": {
        "id": "a-Rx21SKtFF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude 'x' and 'y' columns from the DataFrame\n",
        "columns_to_exclude = ['x', 'y']\n",
        "df_filtered = final_combined_df.drop(columns=columns_to_exclude)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "df_final = pd.DataFrame(scaler.fit_transform(df_filtered), columns=df_filtered.columns)"
      ],
      "metadata": {
        "id": "MGjF781XtVU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing data"
      ],
      "metadata": {
        "id": "d0KL4zQ9u68L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjbb-kIEgkBY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the X and y\n",
        "X = df_final[['VH', 'VV', 'CHIRPS', 'ET', 'DEM', 'SLOPE', 'TWI', 'LST', 'NDVI', 'SAVI', 'NDWI', 'LAI', 'Crop', 'LC', 'EVI', 'OM', 'BD', 'Sand', 'Silt', 'Clay']].values\n",
        "y = df_final[['SMAP']].values\n",
        "\n",
        "# KFold Cross-validation setup\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize arrays to store the results for each fold (loss and MSE)\n",
        "dnn_loss, dnn_mse = [], []\n",
        "cnn_loss, cnn_mse = [], []\n",
        "lstm_loss, lstm_mse = [], []\n",
        "ensemble_dnn_loss, ensemble_dnn_mse = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters optimization"
      ],
      "metadata": {
        "id": "bGXiTFrKLOn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DNN model function\n",
        "def build_dnn_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('num_layers', 1, 16)):\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=1024, step=32),\n",
        "                        activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid', 'elu', 'softmax', 'exponential'])))\n",
        "        model.add(Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "# Define the CNN model function\n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('num_convolutions', 1, 4)):\n",
        "        model.add(Conv1D(filters=hp.Int('filters_' + str(i), min_value=32, max_value=256, step=16),\n",
        "                         kernel_size=hp.Int('kernel_size', 3, 5, step=1),\n",
        "                         activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid', 'elu', 'softmax', 'exponential']),\n",
        "                         input_shape=(X_train.shape[1], 1) if i == 0 else None))\n",
        "        model.add(MaxPooling1D(pool_size=hp.Int('pooling_size', 2, 4, step=1)))\n",
        "    model.add(Flatten())\n",
        "    for j in range(hp.Int('num_dense_layers', 1, 16)):\n",
        "        model.add(Dense(units=hp.Int('dense_nodes_' + str(j), min_value=32, max_value=1024, step=32),\n",
        "                        activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid', 'elu', 'softmax', 'exponential'])))\n",
        "        model.add(Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "# Define the LSTM model function\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('num_lstm_layers', 1, 16)):\n",
        "        model.add(LSTM(units=hp.Int('units_' + str(i), min_value=32, max_value=256, step=16),\n",
        "                       return_sequences=(i < hp.Int('num_lstm_layers', 1, 16) - 1)))\n",
        "        model.add(Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "# Define the Ensemble DNN model function\n",
        "def build_ensemble_dnn_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('num_layers', 1, 16)):\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=1024, step=32),\n",
        "                        activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid', 'elu', 'softmax', 'exponential'])))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "RWyPhVZMCwYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining search space"
      ],
      "metadata": {
        "id": "-jfZu2N5KEyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner as kt\n",
        "\n",
        "#DNN tuning\n",
        "tuner_dnn = kt.BayesianOptimization(\n",
        "    build_dnn_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=50,\n",
        "    executions_per_trial=1,\n",
        "    directory='md',\n",
        "    project_name='dnn_bayesian_optimization'\n",
        ")\n",
        "\n",
        "#CNN tuning\n",
        "tuner_cnn = kt.BayesianOptimization(\n",
        "    build_cnn_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=50,\n",
        "    executions_per_trial=1,\n",
        "    directory='md',\n",
        "    project_name='cnn_bayesian_optimization'\n",
        ")\n",
        "\n",
        "#LSTM tuning\n",
        "tuner_lstm = kt.BayesianOptimization(\n",
        "    build_lstm_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=50,\n",
        "    executions_per_trial=1,\n",
        "    directory='md',\n",
        "    project_name='lstm_bayesian_optimization'\n",
        ")\n",
        "\n",
        "#Ensemble DNN tuning\n",
        "tuner_ensemble_dnn = kt.BayesianOptimization(\n",
        "    build_ensemble_dnn_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=50,\n",
        "    executions_per_trial=1,\n",
        "    directory='md',\n",
        "    project_name='ensemble_dnn_bayesian_optimization'\n",
        ")"
      ],
      "metadata": {
        "id": "_PXTpf3kKdGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the 10 folds for cross-validation\n",
        "for train_index, val_index in kf.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Reshape y_train and y_val\n",
        "    y_train = y_train.reshape(-1)\n",
        "    y_val = y_val.reshape(-1)"
      ],
      "metadata": {
        "id": "A2VzABIJcpNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Run the hyperparameter search for DNN\n",
        "    tuner_dnn.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
        "    best_dnn_model = tuner_dnn.get_best_models(num_models=1)[0]\n",
        "    loss_dnn, mse_dnn = best_dnn_model.evaluate(X_val, y_val)\n",
        "    dnn_loss.append(loss_dnn)\n",
        "    dnn_mse.append(mse_dnn)\n",
        "\n",
        "    # Run the hyperparameter search for CNN\n",
        "    tuner_cnn.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
        "    best_cnn_model = tuner_cnn.get_best_models(num_models=1)[0]\n",
        "    loss_cnn, mse_cnn = best_cnn_model.evaluate(X_val, y_val)\n",
        "    cnn_loss.append(loss_cnn)\n",
        "    cnn_mse.append(mse_cnn)\n",
        "\n",
        "    # Run the hyperparameter search for LSTM\n",
        "    tuner_lstm.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
        "    best_lstm_model = tuner_lstm.get_best_models(num_models=1)[0]\n",
        "    loss_lstm, mse_lstm = best_lstm_model.evaluate(X_val, y_val)\n",
        "    lstm_loss.append(loss_lstm)\n",
        "    lstm_mse.append(mse_lstm)\n",
        "\n",
        "    # Run the hyperparameter search for Ensemble DNN\n",
        "    tuner_ensemble_dnn.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
        "    best_ensemble_dnn_model = tuner_ensemble_dnn.get_best_models(num_models=1)[0]\n",
        "    loss_ensemble_dnn, mse_ensemble_dnn = best_ensemble_dnn_model.evaluate(X_val, y_val)\n",
        "    ensemble_dnn_loss.append(loss_ensemble_dnn)\n",
        "    ensemble_dnn_mse.append(mse_ensemble_dnn)\n",
        "\n",
        "# After all the folds, calculate average loss and mse for each model\n",
        "print(f\"DNN - Average Loss: {np.mean(dnn_loss)}\")\n",
        "print(f\"DNN - Average MSE: {np.mean(dnn_mse)}\")\n",
        "\n",
        "print(f\"CNN - Average Loss: {np.mean(cnn_loss)}\")\n",
        "print(f\"CNN - Average MSE: {np.mean(cnn_mse)}\")\n",
        "\n",
        "print(f\"LSTM - Average Loss: {np.mean(lstm_loss)}\")\n",
        "print(f\"LSTM - Average MSE: {np.mean(lstm_mse)}\")\n",
        "\n",
        "print(f\"Ensemble DNN - Average Loss: {np.mean(ensemble_dnn_loss)}\")\n",
        "print(f\"Ensemble DNN - Average MSE: {np.mean(ensemble_dnn_mse)}\")"
      ],
      "metadata": {
        "id": "D5E3bHj_a8Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final model"
      ],
      "metadata": {
        "id": "_z5dJSmVdKCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_dnn_model():\n",
        "    model = Sequential()\n",
        "    activations = ['elu', 'relu', 'tanh', 'softmax']\n",
        "    for i in range(17):\n",
        "        model.add(Dense(64, activation=activations[i % len(activations)]))\n",
        "        model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=160, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    for _ in range(7):\n",
        "        model.add(LSTM(32, return_sequences=True, dropout=0.15, input_shape=input_shape))\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "def build_ensemble_dnn_model():\n",
        "    model = Sequential()\n",
        "    activations = ['relu', 'tanh', 'softmax']\n",
        "    for i in range(6):\n",
        "        model.add(Dense(64, activation=activations[i % len(activations)]))\n",
        "        model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model\n",
        "\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "\n",
        "# Train Base DNN Model\n",
        "dnn_model = build_dnn_model()\n",
        "dnn_history = dnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Train CNN Model\n",
        "cnn_model = build_cnn_model(X_train_reshaped.shape[1:])\n",
        "cnn_history = cnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, y_val))\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model = build_lstm_model(X_train_reshaped.shape[1:])\n",
        "lstm_history = lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, y_val))\n",
        "\n",
        "# Train Ensemble DNN Model\n",
        "ensemble_dnn_model = build_ensemble_dnn_model()\n",
        "ensemble_dnn_history = ensemble_dnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "ensemble_dnn_loss, ensemble_dnn_mse = ensemble_dnn_model.evaluate(X_val, y_val)\n",
        "print(f\"Ensemble DNN Loss: {ensemble_dnn_loss}, Ensemble DNN MSE: {ensemble_dnn_mse}\")"
      ],
      "metadata": {
        "id": "LvSxLbvTdJR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "ensemble_dnn_model.save('/content/drive/MyDrive/SMAP_DOWNSCALING_CV/model')"
      ],
      "metadata": {
        "id": "4Jsd-rrdRsRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAZD2G1-VAo6"
      },
      "source": [
        "Predicting in the new data at a resolution of 30m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6WgtDKZvh5y"
      },
      "outputs": [],
      "source": [
        "data_for_prediction = final_df_30m[['x', 'y' ,'VH', 'VV', 'CHIRPS','ET','DEM','SLOPE','TWI','NDVI','EVI','NDWI','SAVI','LAI','CROP','LC']]\n",
        "data_for_prediction.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB40Atlavh3c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = data_for_prediction[['VV','VH','CHIRPS','ET','DEM','SLOPE' ,'TWI','NDVI','EVI','NDWI','SAVI','LAI','CROP','LC']]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape X_scaled to add a dimension for channels\n",
        "X_scaled_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9OXBjGdAFGv"
      },
      "outputs": [],
      "source": [
        "#loading the trained model\n",
        "import keras\n",
        "model = keras.models.load_model('/content/drive/MyDrive/SMAP_DOWNSCALING_CV/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwrVJZYdvh0y"
      },
      "outputs": [],
      "source": [
        "#predict on the new data\n",
        "y_pred = model.predict(X_scaled_reshaped)\n",
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJG_UtbPvhyj"
      },
      "outputs": [],
      "source": [
        "final_df_30m['SMAP'] = y_pred\n",
        "final_df_30m = final_df_30m[['x', 'y', 'VH', 'VV','CHIRPS','ET','DEM','SLOPE','NDVI','EVI','NDWI','SAVI','LAI','CROP','LC', 'SMAP']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30aTXbMXi6FJ"
      },
      "outputs": [],
      "source": [
        "final_df_30m_new = final_df_30m[['x', 'y', 'SMAP']]\n",
        "final_df_30m_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs8H1VR3oCxJ"
      },
      "outputs": [],
      "source": [
        "# Determine the original rows and columns\n",
        "original_rows = final_df_30m_new['y'].nunique()\n",
        "original_cols = final_df_30m_new['x'].nunique()\n",
        "original_smap = final_df_30m_new['SMAP'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSb3JKrmXSFV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the 'SMAP' values\n",
        "plt.scatter(final_df_30m_new['x'], final_df_30m_new['y'], c=final_df_30m_new['SMAP'], cmap='viridis')\n",
        "plt.colorbar(label='SMAP')\n",
        "\n",
        "# Add labels and title to the plot\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('SMAP Predictions')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}